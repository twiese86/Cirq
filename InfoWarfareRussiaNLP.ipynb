{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InfoWarfareRussiaNLP.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNyL6zcihCOQPGvueU/rqWT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/twiese86/Cirq/blob/master/InfoWarfareRussiaNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab3ertnK8N5H"
      },
      "source": [
        "#**Information Warfare**\n",
        "By Thomas L. Wiese III\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyAXnkWhNoiZ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import collections\n",
        "\n",
        "import tweepy as tw\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1La8Zp9Opaql"
      },
      "source": [
        "url1 = 'https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_1.csv'\n",
        "url2 = 'https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_2.csv'\n",
        "url3 = 'https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_3.csv'\n",
        "url4 = 'https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_4.csv'\n",
        "url5 = 'https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_5.csv'\n",
        "url6 = 'https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_6.csv'\n",
        "url7 = 'https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_7.csv'\n",
        "url8 = 'https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_8.csv'\n",
        "url9 = 'https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_9.csv'\n",
        "url10 = 'https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_10.csv'\n",
        "url11 = 'https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_11.csv'\n",
        "url12 = 'https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_12.csv'\n",
        "url13 = 'https://raw.githubusercontent.com/fivethirtyeight/russian-troll-tweets/master/IRAhandle_tweets_13.csv'\n",
        "\n",
        "#datasource: https://www.tandfonline.com/doi/abs/10.1080/10584609.2020.1718257\n",
        "#Darren L. Linvill & Patrick L. Warren (2020) Troll Factories: Manufacturing Specialized Disinformation on Twitter, Political Communication, 37:4, 447-467, DOI: 10.1080/10584609.2020.1718257\n"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWDFXagwqTJr"
      },
      "source": [
        "df1 = pd.read_csv(url1,index_col=0,parse_dates=[0]) #read csv data files\n",
        "df2 = pd.read_csv(url2,index_col=0,parse_dates=[0])\n",
        "df3 = pd.read_csv(url3,index_col=0,parse_dates=[0])\n",
        "df4 = pd.read_csv(url4,index_col=0,parse_dates=[0])\n",
        "df5 = pd.read_csv(url5,index_col=0,parse_dates=[0])\n",
        "df6 = pd.read_csv(url6,index_col=0,parse_dates=[0])\n",
        "df7 = pd.read_csv(url7,index_col=0,parse_dates=[0])\n",
        "df8 = pd.read_csv(url8,index_col=0,parse_dates=[0])\n",
        "df9 = pd.read_csv(url9,index_col=0,parse_dates=[0])\n",
        "df10 = pd.read_csv(url10,index_col=0,parse_dates=[0])\n",
        "df11 = pd.read_csv(url11,index_col=0,parse_dates=[0])\n",
        "df12 = pd.read_csv(url12,index_col=0,parse_dates=[0])\n",
        "df13 = pd.read_csv(url13,index_col=0,parse_dates=[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqWkpoxMrpdN"
      },
      "source": [
        "df14 = df1.append(df2)    #append all the datasets into a single dataframe\n",
        "df15 = df14.append(df3)\n",
        "df16 = df15.append(df4)\n",
        "df17 = df16.append(df5)\n",
        "df18 = df17.append(df6)\n",
        "df19 = df18.append(df7)\n",
        "df20 = df19.append(df9)\n",
        "df21 = df20.append(df10)\n",
        "df22 = df21.append(df11)\n",
        "df23 = df22.append(df12)\n",
        "df24 = df23.append(df13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5AsmXslxFG4"
      },
      "source": [
        "df24.info() #review datatypes and column names\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6TlP38ev3Yd"
      },
      "source": [
        "df24.head() #review a sample fo the data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucrvQqfZVOiC"
      },
      "source": [
        "df24.isnull().sum()\n",
        "# this tells me which columns have null values and how many"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "011CfD8g62ki"
      },
      "source": [
        "### **Notes on Null Values from Data Dictionary**\n",
        "\n",
        "\n",
        "1.   region - A region classification, as determined by Social Studio\n",
        "2.   post_type\t- Indicates if the tweet was a retweet or a quote-tweet\n",
        "3.   tco#_step1 - tco1_step1\tFirst redirect for the first http(s)://t.co/ link in a tweet, if it exists\n",
        "tco2_step1\tFirst redirect for the second http(s)://t.co/ link in a tweet, if it exists\n",
        "tco3_step1\tFirst redirect for the third http(s)://t.co/ link in a tweet, if it exists.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvMW6_96VOiH"
      },
      "source": [
        "# Now I'm just going to drop things that are obviously usless but don't nessecarily have null values\n",
        "df24.drop(['account_type','account_category','article_url'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzTtDK5oFO6C"
      },
      "source": [
        "## Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b1O8X4_F381"
      },
      "source": [
        "df24"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPi01kvlK69U"
      },
      "source": [
        "sb.set_context({\"figure.figsize\": (50, 25)})\n",
        "plot = sb.countplot(df24['author'], order=df24.author.value_counts().iloc[:55].index)\n",
        "plt.xticks(rotation=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kqjK5feF8Mc"
      },
      "source": [
        "import string\n",
        "string.punctuation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PV4BycW8Gb7P"
      },
      "source": [
        "df24"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6nLrwuKH86-"
      },
      "source": [
        "df24['language'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uC9aYj_I1qM"
      },
      "source": [
        "df24[df24[\"language\"].str.contains(\"English\")==True] #drops all non-english tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj-4YApZNmSe"
      },
      "source": [
        "!pip install tweet-preprocessor\n",
        "import preprocessor as p\n",
        "# https://www.kaggle.com/sreejiths0/efficient-tweet-preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXE3mjT5NmyR"
      },
      "source": [
        "train_df = df24.dropna()\n",
        "train_df = df24.drop_duplicates()\n",
        "train_df.rename(columns = {'content' : 'text'}, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unrn9V0ZN2R9"
      },
      "source": [
        "def preprocess_tweet(row):\n",
        "    text = row['text']\n",
        "    text = p.clean(text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJDOuxT7OHfz"
      },
      "source": [
        "train_df['text']=train_df['text'].apply(str)\n",
        "train_df['text'] = train_df.apply(preprocess_tweet, axis=1) \n",
        "\n",
        "#Tweets have been cleaned to normal text.\n",
        "#Lowercasing\n",
        "#Punctuation Removal\n",
        "#Replace extra white spaces\n",
        "#Stopwords removal\n",
        "#For stop word removal , i have used gensim library\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJoW27DwOOCa"
      },
      "source": [
        "train_df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzUloD1OT3XI"
      },
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21HxVpyqUBDF"
      },
      "source": [
        "def stopword_removal(row):\n",
        "    text = row['text']\n",
        "    text = remove_stopwords(text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rloHsRHqUEY8"
      },
      "source": [
        "train_df['text'] = train_df.apply(stopword_removal, axis=1) # remove stopwords\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4jg-35_UHb0"
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXVbSTW3UMyT"
      },
      "source": [
        "train_df['text'] = train_df['text'].str.lower().str.replace('[^\\w\\s]',' ').str.replace('\\s\\s+', ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_oCB2UFUfcu"
      },
      "source": [
        "train_df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zWhxIYKUieg"
      },
      "source": [
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "def show_wordcloud(data, title = None):\n",
        "    wordcloud = WordCloud(\n",
        "        background_color='white',\n",
        "        stopwords=stopwords,\n",
        "        max_words=200,\n",
        "        max_font_size=40, \n",
        "        scale=3,\n",
        "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
        "    ).generate(str(data))\n",
        "\n",
        "    fig = plt.figure(1, figsize=(12, 12))\n",
        "    plt.axis('off')\n",
        "    if title: \n",
        "        fig.suptitle(title, fontsize=20)\n",
        "        fig.subplots_adjust(top=2.3)\n",
        "\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.show()\n",
        "\n",
        "show_wordcloud(train_df['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jynk3gKMai4L"
      },
      "source": [
        "corpus=[]\n",
        "new= train_df['text'].str.split()\n",
        "new=new.values.tolist()\n",
        "corpus=[word for i in new for word in i] #creates a corpus of twittet words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA0-vRMqfKK_"
      },
      "source": [
        "corpus "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRc83RK_jmSv"
      },
      "source": [
        "data_analysis = nltk.FreqDist(corpus)\n",
        "data_analysis.plot(50, cumulative=False) #seems like there are stop words in here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRcTsJJKl-c1"
      },
      "source": [
        "train_df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOMo-5TVmjnf"
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_SBL20g0TMs"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGk5RzUq0ZYJ"
      },
      "source": [
        "filtered_corpus =[]\n",
        "for w in corpus:\n",
        "  if w not in stop_words:\n",
        "    filtered_corpus.append(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSOMH2n51fGa"
      },
      "source": [
        "data_analysis = nltk.FreqDist(filtered_corpus)\n",
        "data_analysis.plot(20, cumulative=False) #seems like there are stop words in here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdGr53io1g7v"
      },
      "source": [
        "# this looks better to me, fewer words that seem like stop words\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "res = ' '.join(map(str, filtered_corpus))\n",
        "wordcloud = WordCloud().generate(res)\n",
        "#interestingly, the third most common word is police"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y21LmAds2EAa"
      },
      "source": [
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fo8q12N4_vo"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#people tend to believe what their social network is telling them is true, \"friend\", and \"believe\" standout here, mention research into villages and birthcontrol from viral spread of info book"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuaC-Sim7Jo1"
      },
      "source": [
        "import collections\n",
        "def get_top_ngram(corp, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(n, n)).fit(corp)\n",
        "    bag_of_words = vec.transform(corp)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) \n",
        "                  for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:10]\n",
        "top_n_bigrams=get_top_ngram(train_df['text'],2)[:10]\n",
        "x,y=map(list,zip(*top_n_bigrams))\n",
        "sb.barplot(x=y,y=x)    #bigram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brGKv3DG7jdg"
      },
      "source": [
        "#trigram\n",
        "top_tri_grams=get_top_ngram(train_df['text'],n=3)\n",
        "x,y=map(list,zip(*top_tri_grams))\n",
        "sb.barplot(x=y,y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD6M0OArGXSx"
      },
      "source": [
        "#https://paperswithcode.com/dataset/liar\n",
        "#starting the ML, reading in and analyzing the LIAR dataset\n",
        "! pip install fsspec\n",
        "liarurl= 'https://raw.githubusercontent.com/ekagra-ranjan/fake-news-detection-LIAR-pytorch/master/train.tsv'\n",
        "df_liar = pd.read_csv(liarurl, sep='\\t', error_bad_lines=False, names=[\"jsonid\", \"rating\", \"text\", \"metatag\", 'person', 'personcat','state', 'party', '1', '2', '3', '4', '5', 'source'], index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "se6PrThObXVW"
      },
      "source": [
        "df_liar.head()\n",
        "#LIAR: A BENCHMARK DATASET FOR FAKE NEWS DETECTION\n",
        "\n",
        "#William Yang Wang, \"Liar, Liar Pants on Fire\": A New Benchmark Dataset for Fake News Detection, to appear in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017), short paper, Vancouver, BC, Canada, July 30-August 4, ACL.\n",
        "#=====================================================================\n",
        "#Description of the TSV format:\n",
        "\n",
        "#Column 1: the ID of the statement ([ID].json).\n",
        "#Column 2: the label.\n",
        "#Column 3: the statement.\n",
        "#Column 4: the subject(s).\n",
        "#Column 5: the speaker.\n",
        "#Column 6: the speaker's job title.\n",
        "#Column 7: the state info.\n",
        "#Column 8: the party affiliation.\n",
        "#Column 9-13: the total credit history count, including the current statement.\n",
        "#9: barely true counts.\n",
        "#10: false counts.\n",
        "#11: half true counts.\n",
        "#12: mostly true counts.\n",
        "#13: pants on fire counts.\n",
        "#Column 14: the context (venue / location of the speech or statement).\n",
        "\n",
        "#Note that we do not provide the full-text verdict report in this current version of the dataset,\n",
        "#but you can use the following command to access the full verdict report and links to the source documents:\n",
        "#wget http://www.politifact.com//api/v/2/statement/[ID]/?format=json\n",
        "\n",
        "#======================================================================\n",
        "#The original sources retain the copyright of the data.\n",
        "\n",
        "#Note that there are absolutely no guarantees with this data,\n",
        "#and we provide this dataset \"as is\",\n",
        "#but you are welcome to report the issues of the preliminary version\n",
        "#of this data.\n",
        "\n",
        "#You are allowed to use this dataset for research purposes only.\n",
        "\n",
        "#For more question about the dataset, please contact:\n",
        "#William Wang, william@cs.ucsb.edu\n",
        "\n",
        "#v1.0 04/23/2017\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNqJJKoxfive"
      },
      "source": [
        "# i dont know what columns 1-5 actually represent, need to figure that our\n",
        "df_rsntwts = train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FeKP4wARfto9"
      },
      "source": [
        "df_liar.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHfsM7OgfwYz"
      },
      "source": [
        "df_liar['rating'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSZNzk22_ac0"
      },
      "source": [
        "df_liar.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fi-VEiR-iRq"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "lb_make = LabelEncoder()\n",
        "df_liar['ratingnum'] = lb_make.fit_transform(df_liar['rating'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SGF5CbM-l2z"
      },
      "source": [
        "df_liar['text'] = df_liar.apply(stopword_removal, axis=1) # remove stopwords\n",
        "#train_df['text']=train_df['text'].apply(str)\n",
        "df_liar['text'] = df_liar.apply(preprocess_tweet, axis=1) \n",
        "#make corpus of liar text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhgG4gleIOsP"
      },
      "source": [
        "df_liar.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehtVBEbkJK3p"
      },
      "source": [
        "import string\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2upTEcCtKCKX"
      },
      "source": [
        "df_liar[\"text_clean\"] = df_liar['text'].str.replace('[^\\w\\s]','')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPEToanDKdUU"
      },
      "source": [
        "df_liar.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ8NnCLFKkas"
      },
      "source": [
        "df_liar[\"text_clean\"] = df_liar[\"text_clean\"].str.lower()\n",
        "df_liar.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wJjSv5mMb31"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = df_liar['text_clean']\n",
        "y = df_liar['ratingnum']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coE80NPyPIC6"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_final = vectorizer.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8BHj1-fPQIX"
      },
      "source": [
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
        "                      ('clf', LinearSVC()),\n",
        "                    ])\n",
        "text_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW1_-bkhPfk_"
      },
      "source": [
        "predictions = text_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_Xflx5pP3gc"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "print(accuracy_score(y_test, predictions))\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-CXYo1gP7Yk"
      },
      "source": [
        "# make everything < mostly true = false"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QjlXkfWTwB9"
      },
      "source": [
        "df_liar['ratingnum'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asGEfQFsT3Qh"
      },
      "source": [
        "df_liar['rating'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG-3gfdDWLPd"
      },
      "source": [
        "df_liar['ratingnum'].mask(df_liar['ratingnum'] == 3, 5, inplace=True) # change mostly-true to true\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvkR-8eVYj94"
      },
      "source": [
        "df_liar['ratingnum'].mask(df_liar['ratingnum'] == 2, 1, inplace=True)\n",
        "df_liar['ratingnum'].mask(df_liar['ratingnum'] == 4, 1, inplace=True)\n",
        "df_liar['ratingnum'].mask(df_liar['ratingnum'] == 0, 1, inplace=True) #change anything <mostly-true to false\n",
        "# key: 5 = true, 1 = false"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-WHlB5FY8km"
      },
      "source": [
        "df_liar.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv-_xngxY_fU"
      },
      "source": [
        "df_liar['ratingnum'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gm90JtI4ZCo8"
      },
      "source": [
        "X = df_liar['text_clean']\n",
        "y = df_liar['ratingnum']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpxTa7kWZN8a"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "X_train_final = vectorizer.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhfX8hFeZR2h"
      },
      "source": [
        "#LinearSVC_classifier = SklearnClassifier(SVC(kernel='linear',probability=True)), LinearSVC()\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
        "                      ('clf', LinearSVC()),\n",
        "                    ])\n",
        "text_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgxz44kAZVSJ"
      },
      "source": [
        "predictions = text_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN5V54SxZY64"
      },
      "source": [
        "print(accuracy_score(y_test, predictions))\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8s1tgqQCZcjI"
      },
      "source": [
        "#prediction is 61% accurate for detecting truth vs lie within LIAR dataset\n",
        "#https://medium.com/@hritikattri10/feature-extraction-using-tf-idf-algorithm-44eedb37305e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZKShekrkWC_"
      },
      "source": [
        "df_rsntwts.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3i8QL0ykceF"
      },
      "source": [
        "df_rsntwts['ratingnum'] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNTQopvRk-N2"
      },
      "source": [
        "df_rsntwts.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DWR3yqLl3oO"
      },
      "source": [
        "X1 = df_rsntwts['text']\n",
        "Y1 = df_rsntwts['ratingnum']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q_bHJdjmlTT"
      },
      "source": [
        "predictions = text_clf.predict(X1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E95fBil2mYgN"
      },
      "source": [
        "print(accuracy_score(Y1, predictions))\n",
        "print(confusion_matrix(Y1, predictions))\n",
        "print(classification_report(Y1, predictions))#predict if IRA tweets are false using the LIAR dataset assuming all IRA tweets are 'false'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1HzCiSI60QR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyzTZRzsunHL"
      },
      "source": [
        "import tensorflow as tf  #now we are going to use RNN rather than SVC\n",
        "import re \n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
        "import seaborn as sns \n",
        "plt.style.use('ggplot')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wpvPD0a6oiR"
      },
      "source": [
        "max_vocab = 10000\n",
        "tokenizer = Tokenizer(num_words=max_vocab)\n",
        "tokenizer.fit_on_texts(X_train) #https://www.kaggle.com/therealcyberlord/fake-news-detection-using-rnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gesR6MGH7Owy"
      },
      "source": [
        "# tokenize the text into vectors \n",
        "X_train = tokenizer.texts_to_sequences(df_liar['text_clean'] #this is only 80% of the LIAR data, lets try with 100%\n",
        "X_test = tokenizer.texts_to_sequences(X1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHToJAcA7T-R"
      },
      "source": [
        "y_test = Y1 #similar, lets convert this to ALL LIAR data\n",
        "y_train = df_liar['ratingnum']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJnl1aEC7frw"
      },
      "source": [
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=256)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, padding='post', maxlen=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDi77H1V7qz9"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(max_vocab, 32),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTADzNJD7wK1"
      },
      "source": [
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10,validation_split=0.1, batch_size=30, shuffle=True, callbacks=[early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBrgVcVv72pF"
      },
      "source": [
        "history_dict = history.history\n",
        "\n",
        "acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "epochs = history.epoch\n",
        "\n",
        "plt.figure(figsize=(12,9))\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss', size=20)\n",
        "plt.xlabel('Epochs', size=20)\n",
        "plt.ylabel('Loss', size=20)\n",
        "plt.legend(prop={'size': 20})\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,9))\n",
        "plt.plot(epochs, acc, 'g', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy', size=20)\n",
        "plt.xlabel('Epochs', size=20)\n",
        "plt.ylabel('Accuracy', size=20)\n",
        "plt.legend(prop={'size': 20})\n",
        "plt.ylim((0.5,1))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnLiduqnGJ5e"
      },
      "source": [
        "#Evaluate the testing set # dont run this\n",
        "\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xp-O5rUGUnc"
      },
      "source": [
        "pred = model.predict(X_test) #dont run this\n",
        "\n",
        "binary_predictions = []\n",
        "\n",
        "for i in pred:\n",
        "    if i >= 0.5:\n",
        "        binary_predictions.append(1)\n",
        "    else:\n",
        "        binary_predictions.append(0) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRXkuV0uGcV5"
      },
      "source": [
        "print('Accuracy on testing set:', accuracy_score(binary_predictions, y_test)) #sont run this\n",
        "print('Precision on testing set:', precision_score(binary_predictions, y_test))\n",
        "print('Recall on testing set:', recall_score(binary_predictions, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m6dQzByGhTX"
      },
      "source": [
        "matrix = confusion_matrix(binary_predictions, y_test, normalize='all')#dont run this\n",
        "plt.figure(figsize=(16, 10))\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(matrix, annot=True, ax = ax)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted Labels', size=20)\n",
        "ax.set_ylabel('True Labels', size=20)\n",
        "ax.set_title('Confusion Matrix', size=20) \n",
        "ax.xaxis.set_ticklabels([0,1], size=15)\n",
        "ax.yaxis.set_ticklabels([0,1], size=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx8jlXr0G1hm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}